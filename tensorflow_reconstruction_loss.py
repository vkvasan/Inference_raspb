# -*- coding: utf-8 -*-
"""tensorflow_reconstruction_loss.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Uq3_Fe_lpUWisSJ-eO9Z_O4nMZuI-lsu
"""

import torch
from torch import nn, optim
from torch.nn import functional as F
import argparse
import datetime
import os
from copy import deepcopy
from tqdm import tqdm 
import numpy as np
from matplotlib import pyplot as plt
from torch.utils.tensorboard import SummaryWriter
#import wandb
import torchvision.utils as vutils
from statsmodels.tsa.seasonal import STL
from scipy.signal import medfilt2d
import tensorflow
from tensorflow.keras.layers import Input 
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

from qkeras import QDense, QActivation, QConv1D
from qkeras.quantizers import quantized_bits, quantized_relu, smooth_sigmoid
from qkeras.utils import model_save_quantized_weights

#np only 
def DataBatch(data, label, text, batchsize, shuffle=True):
    
    n = data.shape[0]
    if shuffle:
        index = np.random.permutation(n)
    else:
        index = np.arange(n)
    for i in range(int(np.ceil(n/batchsize))):
        inds = index[i*batchsize : min(n,(i+1)*batchsize)]
        yield data[inds], label[inds], text[inds]


#np and torch 
def sample_noise(data, rate):
    a = int( data.shape[0] )
    b = int( data.shape[1] )
    c = int( data.shape[2] )
    #print( data.shape[0])
    noise = 0.1 * np.random.rand(a, b, c)
   
    idx1 = np.random.choice(data.shape[1], int(rate*data.shape[1]))
    noise_data = deepcopy(data)
    noise_data[:,idx1] = data[:,idx1] + noise[:,idx1]

    idx3 = np.random.choice(range(1,data.shape[1]), int(rate*(data.shape[1]-1)))
    noise_data[:,idx3] = noise_data[:,idx3-1]
    
    return noise_data, np.concatenate((idx1,idx3))


z = 10
epochs = 5001
bs = 16
seq_len = 3000
logdir = '/log'
model_path = '/checkpoint'
run_tag = ''
train = 1
lam = 0.5 
rate = 0.01
batchSize = 16 

# load dataset
high_vi = tf.convert_to_tensor(np.load('./data/train/high_vi_%d.npy'%seq_len))
print( high_vi.dtype)
high_w = tf.convert_to_tensor(np.load('./data/train/high_w_%d.npy'%seq_len))
print( high_w.dtype)

high_a = tf.convert_to_tensor(np.load('./data/train/high_a_%d.npy'%seq_len))
print( high_a.dtype)

low_vi = tf.convert_to_tensor(np.load('./data/test/low_vi_%d.npy'%seq_len))
low_w = tf.convert_to_tensor(np.load('./data/test/low_w_%d.npy'%seq_len))
low_a = tf.convert_to_tensor(np.load('./data/test/low_a_%d.npy'%seq_len))

seq_len, vi_dim, imu_w_dim, imu_a_dim = high_vi.shape[1], high_vi.shape[2], high_w.shape[2], high_a.shape[2]

high_vi = high_vi.numpy()
high_w = high_w.numpy()
high_a = high_a.numpy()



in_dim = vi_dim+imu_w_dim+imu_a_dim
z_dim = z
out_dim=vi_dim

inputs = Input(shape=[seq_len, in_dim  ], name='input')
print(inputs.shape)
x = QConv1D( kernel_quantizer=quantized_bits(6,0,alpha=1), bias_quantizer=quantized_bits(6,0,alpha=1),filters=128, kernel_size=7, padding='causal')(inputs)
print(x.shape)
x = QActivation(activation=quantized_relu(6,0))(x)
x = QConv1D( kernel_quantizer=quantized_bits(6,0,alpha=1), bias_quantizer=quantized_bits(6,0,alpha=1), filters=256, kernel_size=5, padding='causal')(x)
x = QActivation(activation=quantized_relu(6,0))(x)
print(x.shape)
x = QConv1D( kernel_quantizer=quantized_bits(6,0,alpha=1), bias_quantizer=quantized_bits(6,0,alpha=1), filters=128, kernel_size=3, padding='causal')(x)
x = QActivation(activation=quantized_relu(6,0))(x)
print(x.shape)
x = QConv1D( kernel_quantizer=quantized_bits(6,0,alpha=1), bias_quantizer=quantized_bits(6,0,alpha=1), filters=out_dim, kernel_size=3, padding='causal')(x)
print(x.shape)
out = x


model = Model(inputs, out)

model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])

from tensorflow.keras.losses import mean_squared_error
mse = tf.keras.losses.MeanSquaredError()

for epoch in range(100):

    total_loss, total_vi_loss, total_w_loss, total_a_loss = 0, 0, 0, 0
    for batch_vi, batch_w, batch_a in DataBatch(high_vi, high_w, high_a, batchSize):
        noise_vi, idx1 = sample_noise(batch_vi, rate)
        input_model = noise_vi, batch_w, batch_a
        #print(idx1)
        #print( "noise_vi", noise_vi.shape)
        #print( "batch_w", batch_w.shape)
        #print( "batch_a", batch_a.shape)
        x_batch_noisy = tf.concat([noise_vi,batch_w,batch_a],axis=-1)
        with tf.GradientTape() as tape:
            predictions = model(x_batch_noisy, training=True)
            #print( "predictions", predictions.shape)
            #print( "batch_vi", batch_vi.shape)
            #a = tf.transpose(  predictions, perm=[0 ,2 ,1] )
            #b = tf.transpose(  batch_vi, perm=[0 ,2 ,1] )
            #print( "a", a.shape)
            #print( "b", b.shape)
            #predictions = predictions.numpy()
            #batch_vi = batch_vi.numpy()
            idx1 = tf.convert_to_tensor(idx1)
            
            prediction_new = tf.gather(predictions, idx1, axis=1)
            #print(prediction_new.shape)
            #prediction_new = predictions[:,idx1_np,:]
            batch_vi_new = batch_vi[:,idx1,:]
            #prediction_new = tf.convert_to_tensor(prediction_new)
            #batch_vi_new = tf.convert_to_tensor(batch_vi_new)
            loss_value = mse( prediction_new , batch_vi_new )
       
        grads = tape.gradient(loss_value, model.trainable_weights)
        model.optimizer.apply_gradients(zip(grads, model.trainable_weights))    
        total_loss += int(batch_vi.shape[0]) * loss_value
        #model.train_on_batch(x_batch_noisy, batch_vi)
    
    avg_epoch_loss = total_loss/len(high_vi)
    print('[%d]Training loss:%.4f', epoch, avg_epoch_loss)

testgenerator_x = tf.concat([low_vi, low_w, low_a],axis=-1)
testgenerator_y = low_vi
print(testgenerator_x.shape,testgenerator_y.shape)

single_sample_x = testgenerator_x[0,:,:]
single_sample_y = testgenerator_y[0,:,:]
print(single_sample_x.shape,single_sample_y.shape)

n_batches = int(testgenerator_x.shape[0]/batchSize)
print( n_batches )
test_loss = model.evaluate(testgenerator_x, testgenerator_y, batch_size=batchSize)

#loss = model.evaluate([testgenerator_x,testgenerator_y], steps=n_batches , verbose=2)

import time

start_time = time.time()
predictions = model.predict(testgenerator_x)
end_time = time.time()

# Calculate inference time
inference_time = end_time - start_time
average_inference_time = inference_time / 460

print("Total inference time:", inference_time, "seconds")
print("Average inference time per sample:", average_inference_time, "seconds")

for layer in model.layers:
    layer_name = layer.name
    layer_weights = layer.get_weights()
    if len(layer_weights) > 0:
        print(f"Layer: {layer_name}")
        for i, weight in enumerate(layer_weights):
            print(f"Weight {i + 1}:")
            print(weight)
            print()

#converter = tf.lite.TFLiteConverter.from_keras_model(model)
#tflite_model = converter.convert()

# Save the model.
#with open('model.tflite', 'wb') as f:
#  f.write(tflite_model)

#interpreter = tf.lite.Interpreter(model_path="model.tflite")
#interpreter.allocate_tensors()

# Get input and output tensors.
#input_details = interpreter.get_input_details()
#output_details = interpreter.get_output_details()


#print(input_details[0])


# Load the TFLite model and allocate tensors.
#interpreter = tf.lite.Interpreter(model_path="model.tflite")
#interpreter.allocate_tensors()

# Get input and output tensors.
#input_details = interpreter.get_input_details()
#output_details = interpreter.get_output_details()

# Test the model on random input data.
#input_shape = input_details[0]['shape']
#print(input_shape)
#input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
#interpreter.set_tensor(input_details[0]['index'], input_data)
  
#start_time = time.time()
#interpreter.invoke()
#stop_time = time.time()

#print('time: {:.3f}ms'.format((stop_time - start_time) * 1000))

# The function `get_tensor()` returns a copy of the tensor data.
# Use `tensor()` in order to get a pointer to the tensor.
#output_data = interpreter.get_tensor(output_details[0]['index'])
#print(output_data)